{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n",
    "\n",
    "#### Maximum Points: 100\n",
    "\n",
    "<div>\n",
    "    <table>\n",
    "        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n",
    "        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n",
    "        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Dev Scalars Log Link</h3></td> <td><h3>5</h3></td> </tr>\n",
    "        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n",
    "\n",
    "In this section, you have to write a class or methods that will be used to get training and validation data\n",
    "loader.\n",
    "\n",
    "You will have to write a custom dataset class to load data.\n",
    "\n",
    "**Note that there are not separate validation data, so you will have to create your validation set by dividing train data into train and validation data. Usually, in practice, we do `80:20` ratio for train and validation, respectively.** \n",
    "\n",
    "For example,\n",
    "\n",
    "```\n",
    "class KenyanFood13Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "    ....\n",
    "    ...\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "    \n",
    "```\n",
    "\n",
    "```\n",
    "def get_data(args1, *agrs):\n",
    "    ....\n",
    "    ....\n",
    "    return train_loader, test_loader\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "#from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from trainer import Trainer, hooks, configuration\n",
    "from trainer.utils import setup_system, patch_configs\n",
    "from trainer.metrics import AccuracyEstimator\n",
    "from trainer.tensorboard_visualizer import TensorBoardVisualizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KenyanFood13Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This custom dataset class takes root directory, flag, and transform \n",
    "    and returns training dataset if flag is 0, validation dataset if flag is 1 \n",
    "    else it returns test dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_root, flag, split, transform, random_state):\n",
    "        labels_to_idx = {'githeri' : 0, 'ugali' : 1, 'kachumbari' : 2, 'matoke' : 3, \n",
    "                         'sukumawiki' : 4, 'bhaji' : 5, 'mandazi' : 6, \n",
    "                         'kukuchoma' : 7, 'nyamachoma' : 8, 'pilau' : 9, \n",
    "                         'chapati' : 10, 'masalachips' : 11, 'mukimo' : 12}\n",
    "        self.idx_to_labels = {0 : 'githeri', 1 : 'ugali', 2 : 'kachumbari', 3 : 'matoke',\n",
    "                              4 : 'sukumawiki', 5 : 'bhaji', 6 : 'mandazi',\n",
    "                              7 : 'kukuchoma', 8 : 'nyamachoma', 9 : 'pilau',\n",
    "                              10 : 'chapati', 11 : 'masalachips', 12 : 'mukimo'}\n",
    "        self.data_root = data_root\n",
    "        self.transform = transform\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "        self.weights = None\n",
    "        \n",
    "        if flag == 0 or flag == 1:\n",
    "            \n",
    "            data_csv = pd.read_csv(os.path.join(data_root, 'train.csv'))\n",
    "            \n",
    "            food_category = {}\n",
    "            \n",
    "            for idx, row in data_csv.iterrows():\n",
    "                key = row['class']\n",
    "                if key not in food_category.keys():\n",
    "                    food_category[key] = []\n",
    "                food_category[key].append(row['id'])\n",
    "            \n",
    "            num_classes = len(food_category.keys())\n",
    "            assert num_classes == 13\n",
    "            \n",
    "            for category in food_category.keys():\n",
    "                random.seed(random_state)\n",
    "                random.shuffle(food_category[category])\n",
    "                \n",
    "                count = len(food_category[category])\n",
    "                split_idx = math.floor(count*split)\n",
    "                if flag == 0:\n",
    "                    self.imgs.extend(food_category[category][:split_idx])\n",
    "                    count = len(food_category[category][:split_idx])\n",
    "                    self.labels.extend([labels_to_idx[category]]*count)\n",
    "                else:\n",
    "                    self.imgs.extend(food_category[category][split_idx:])\n",
    "                    count = len(food_category[category][split_idx:])\n",
    "                    self.labels.extend([labels_to_idx[category]]*count)\n",
    "                \n",
    "                self.weights = compute_class_weight('balanced', \n",
    "                                                    classes=np.unique(self.labels), \n",
    "                                                    y=self.labels)\n",
    "        else: # flag == 2\n",
    "            data_csv = pd.read_csv(os.path.join(data_root, 'test.csv'))\n",
    "            self.imgs.extend(data_csv['id'])\n",
    "            self.labels = None\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        return length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        For given index, return images with resize and preprocessing.\n",
    "        \"\"\"\n",
    "        img_name = str(self.imgs[idx]) + '.jpg'\n",
    "        img_path = os.path.join(self.data_root, 'images', 'images', img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        target = None\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        if self.labels is not None:\n",
    "            target = self.labels[idx]\n",
    "        else:\n",
    "            target = str(self.imgs[idx])\n",
    "        return image, target\n",
    "\n",
    "    def get_class_weight(self):\n",
    "        return self.weights\n",
    "\n",
    "    def idx_to_labels(self, idx):\n",
    "        return self.idx_to_labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2. Configuration [5 Points]</font>\n",
    "\n",
    "Define your configuration in this section.\n",
    "\n",
    "For example,\n",
    "\n",
    "```\n",
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 10 \n",
    "    epochs_count: int = 50  \n",
    "    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n",
    "    log_interval: int = 5  \n",
    "    test_interval: int = 1  \n",
    "    data_root: str = \"/kaggle/input/pytorch-opencv-course-classification/\" \n",
    "    num_workers: int = 2  \n",
    "    device: str = 'cuda'  \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Callable, Iterable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "@dataclass\n",
    "class SystemConfig:\n",
    "    seed: int = 42  # seed number to set the state of all random number generators\n",
    "    cudnn_benchmark_enabled: bool = False  # enable CuDNN benchmark for the sake of performance\n",
    "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    root_dir: str = \".\"  # dataset directory root\n",
    "    split: float = 0.8\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    resiz = 256\n",
    "    inpsz = 224\n",
    "\n",
    "    train_transforms: Iterable[Callable] = transforms.Compose([\n",
    "        #transforms.Resize(resiz),\n",
    "        #transforms.RandomAffine(degrees=15, translate=(0.25,0.25), scale=(0.5,1.5)),\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomResizedCrop(inpsz),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(0.3,0.3,0.3),\n",
    "        transforms.RandomAffine(degrees=15, translate=(0.25,0.25), scale=(0.5,1.5)),\n",
    "        #transforms.CenterCrop(inpsz),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])  # data transformation to use during training data preparation\n",
    "    test_transforms: Iterable[Callable] = transforms.Compose([\n",
    "        transforms.Resize(resiz),\n",
    "        transforms.CenterCrop(inpsz),\n",
    "        transforms.ToTensor(), # this re-scales image tensor values between 0-1. image_tensor /= 255\n",
    "        # subtract mean (0.485, 0.456, 0.406) and divide by variance (0.229, 0.224, 0.225)\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])  # data transformation to use during test data preparation\n",
    "\n",
    "@dataclass\n",
    "class DataloaderConfig:\n",
    "    batch_size: int = 32  # amount of data to pass through the network at each forward-backward iteration\n",
    "    num_workers: int = 8  # number of concurrent processes using to prepare data\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OptimizerConfig:\n",
    "    learning_rate: float = 0.0001  # determines the speed of network's weights update\n",
    "    momentum: float = 0.9  # used to improve vanilla SGD algorithm and provide better handling of local minimas\n",
    "    weight_decay: float = 0.0001  # amount of additional regularization on the weights values\n",
    "    lr_step_milestones: Iterable = (\n",
    "        30, 40\n",
    "    )  # at which epoches should we make a \"step\" in learning rate (i.e. decrease it in some manner)\n",
    "    lr_gamma: float = 0.1  # multiplier applied to current learning rate at each of lr_step_milestones\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainerConfig:\n",
    "    model_dir: str = \"checkpoints\"  # directory to save model states\n",
    "    model_saving_frequency: int = 1  # frequency of model state savings per epochs\n",
    "    device: str = \"cpu\"  # device to use for training.\n",
    "    epoch_num: int = 50  # number of times the whole dataset will be passed through the network\n",
    "    progress_bar: bool = True  # enable progress bar visualization during train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n",
    "\n",
    "Define methods or classes that will be used in model evaluation, for example, accuracy, f1-score, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same as the one used in Trainer (AccuracyEstimator, BaseMetric) : I did not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AccuracyEstimator(BaseMetric):\n",
    "    def __init__(self, topk=(1, )):\n",
    "        self.topk = topk\n",
    "        self.metrics = [AverageMeter() for i in range(len(topk) + 1)]\n",
    "\n",
    "    def reset(self):\n",
    "        for i in range(len(self.metrics)):\n",
    "            self.metrics[i].reset()\n",
    "\n",
    "    def update_value(self, pred, target):\n",
    "        \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "        with torch.no_grad():\n",
    "            maxk = max(self.topk)\n",
    "            batch_size = target.size(0)\n",
    "\n",
    "            _, pred = pred.topk(maxk, 1, True, True)\n",
    "            pred = pred.t()\n",
    "            correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "            for i, k in enumerate(self.topk):\n",
    "                correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "                self.metrics[i].update(correct_k.mul_(100.0 / batch_size).item())\n",
    "\n",
    "    def get_metric_value(self):\n",
    "        metrics = {}\n",
    "        for i, k in enumerate(self.topk):\n",
    "            metrics[\"top{}\".format(k)] = self.metrics[i].avg\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n",
    "\n",
    "Write the methods or classes that will be used for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same as the one used in Trainer (hooks.py, trainer.py)\n",
    "# I only changed fit to save the epoch # and loss to the model file name as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# **This is a generic class for training loop.**\n",
    "#\n",
    "# Trainer class is equivalent to the `main` method. \n",
    "#\n",
    "# In the main method, we were passing configurations, the model, optimizer, \n",
    "# learning rate scheduler, and the number of epochs.  It was calling the method to get the \n",
    "# train and test data loader. Using these, it is training and validating the model. \n",
    "# During training and validation, it was also sending logs to TensorBoard and saving the model.\n",
    "#\n",
    "# The trainer class is doing the same in a more modular way so that we can experiment with \n",
    "# different loss functions, different visualizers, different types of targets, etc. \n",
    "#\n",
    "\n",
    "\"\"\"Unified class to make training pipeline for deep neural networks.\"\"\"\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from typing import Union, Callable\n",
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "#from tqdm.auto import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from .hooks import test_hook_default, train_hook_default\n",
    "from .visualizer import Visualizer\n",
    "\n",
    "\n",
    "#\n",
    "# Setting different attributes.\n",
    "#\n",
    "# **Parameters:**\n",
    "# - `model` : `nn.Module` - torch model to train\n",
    "# - `loader_train` : `torch.utils.DataLoader` - train dataset loader.\n",
    "# - `loader_test` : `torch.utils.DataLoader` - test dataset loader\n",
    "# - `loss_fn` : `callable` - loss function. In the main function, the cross-entropy loss was \n",
    "# being used; here, we can pass the loss we want to use. \n",
    "# For example, if we are solving a regression problem, we can not use cross-entropy loss. \n",
    "# It is better to use RMS-loss.\n",
    "#\n",
    "# - `metric_fn` : `callable` - evaluation metric function. In the main function, we had loss \n",
    "# and accuracy as our evaluation metric. Here we can pass any evaluation metric. \n",
    "# For example, in a detection problem, we need a precision-recall metric instead of accuracy.\n",
    "#\n",
    "# - `optimizer` : `torch.optim.Optimizer` - Optimizer.\n",
    "# - `lr_scheduler` : `torch.optim.LrScheduler` - Learning Rate scheduler.\n",
    "# - `configuration` : `TrainerConfiguration` - a set of training process parameters.\n",
    "#\n",
    "# Here, we need a data iterator and target iterator separately, because we are writing a \n",
    "# general trainer class. For example, for the detection problem for a single image, \n",
    "# we might have `n`-number of objects and their coordinates. \n",
    "#        \n",
    "# - `data_getter` : `Callable` - function object to extract input data from the \n",
    "# sample prepared by dataloader.\n",
    "#        \n",
    "# - `target_getter` : `Callable` - function object to extract target data from the \n",
    "# sample prepared by dataloader.\n",
    "#        \n",
    "# - `visualizer` : `Visualizer` - optional, shows metrics values (various backends are possible). \n",
    "# We can pass the visualizer of our choice. \n",
    "# For example, Matplotlib based visualizer, TensorBoard based, etc.\n",
    "#\n",
    "# It is also calling its method `_register_default_hooks` what this method does we will see next. \n",
    "# In short, this is making sure that training and validation function is registered at the \n",
    "# time of trainer class object initiation. \n",
    "#\n",
    "# It is calling the another method `register_hook` to register training (`train_hook_default`) \n",
    "# and validation (`test_hook_default`) functions. \n",
    "# `train_hook_default` and `test_hook_default` are defined in the `hook`-module.\n",
    "#\n",
    "# It is updating the key-value pair of a dictionary, where the key is string and value is a \n",
    "# callable function.\n",
    "#\n",
    "# **Parameters:**\n",
    "#\n",
    "# - `hook_type`: `string` - hook type. For example, wether the function will be used for train or test.\n",
    "# - `hook_fn`: `callable` - hook function.\n",
    "#\n",
    "# Taking the number of epochs and training and validating the model. \n",
    "# It is also adding logs to the visualizer. \n",
    "#\n",
    "# **Parameters:**\n",
    "#\n",
    "# - `epochs`: `int` - number of epochs to train model.\n",
    "#\n",
    "\n",
    "class Trainer:  # pylint: disable=too-many-instance-attributes\n",
    "    \"\"\" Generic class for training loop.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        torch model to train\n",
    "    loader_train : torch.utils.DataLoader\n",
    "        train dataset loader.\n",
    "    loader_test : torch.utils.DataLoader\n",
    "        test dataset loader\n",
    "    loss_fn : callable\n",
    "        loss function\n",
    "    metric_fn : callable\n",
    "        evaluation metric function\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer\n",
    "    lr_scheduler : torch.optim.LrScheduler\n",
    "        Learning Rate scheduler\n",
    "    configuration : TrainerConfiguration\n",
    "        a set of training process parameters\n",
    "    data_getter : Callable\n",
    "        function object to extract input data from the sample prepared by dataloader.\n",
    "    target_getter : Callable\n",
    "        function object to extract target data from the sample prepared by dataloader.\n",
    "    visualizer : Visualizer, optional\n",
    "        shows metrics values (various backends are possible)\n",
    "    # \"\"\"\n",
    "    def __init__( # pylint: disable=too-many-arguments\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        loader_train: torch.utils.data.DataLoader,\n",
    "        loader_test: torch.utils.data.DataLoader,\n",
    "        loss_fn: Callable,\n",
    "        metric_fn: Callable,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        lr_scheduler: Callable,\n",
    "        device: Union[torch.device, str] = \"cuda\",\n",
    "        model_saving_frequency: int = 1,\n",
    "        save_dir: Union[str, Path] = \"checkpoints\",\n",
    "        model_name_prefix: str = \"model_\",\n",
    "        data_getter: Callable = itemgetter(\"image\"),\n",
    "        target_getter: Callable = itemgetter(\"target\"),\n",
    "        stage_progress: bool = True,\n",
    "        visualizer: Union[Visualizer, None] = None,\n",
    "        get_key_metric: Callable = itemgetter(\"top1\"),\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.loader_train = loader_train\n",
    "        self.loader_test = loader_test\n",
    "        self.loss_fn = loss_fn\n",
    "        self.metric_fn = metric_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.device = device\n",
    "        self.model_saving_frequency = model_saving_frequency\n",
    "        self.save_dir = save_dir\n",
    "        self.model_name_prefix = model_name_prefix\n",
    "        self.stage_progress = stage_progress\n",
    "        self.data_getter = data_getter\n",
    "        self.target_getter = target_getter\n",
    "        self.hooks = {}\n",
    "        self.visualizer = visualizer\n",
    "        self.get_key_metric = get_key_metric\n",
    "        self.metrics = {\"epoch\": [], \"train_loss\": [], \"test_loss\": [], \"test_metric\": []}\n",
    "        self._register_default_hooks()\n",
    "\n",
    "    def fit(self, epochs):\n",
    "        \"\"\" Fit model method.\n",
    "\n",
    "        Arguments:\n",
    "            epochs (int): number of epochs to train model.\n",
    "        \"\"\"\n",
    "        iterator = tqdm(range(epochs), dynamic_ncols=True)\n",
    "        for epoch in iterator:\n",
    "            output_train = self.hooks[\"train\"](\n",
    "                self.model,\n",
    "                self.loader_train,\n",
    "                self.loss_fn,\n",
    "                self.optimizer,\n",
    "                self.device,\n",
    "                prefix=\"[{}/{}]\".format(epoch, epochs),\n",
    "                stage_progress=self.stage_progress,\n",
    "                data_getter=self.data_getter,\n",
    "                target_getter=self.target_getter\n",
    "            )\n",
    "            output_test = self.hooks[\"test\"](\n",
    "                self.model,\n",
    "                self.loader_test,\n",
    "                self.loss_fn,\n",
    "                self.metric_fn,\n",
    "                self.device,\n",
    "                prefix=\"[{}/{}]\".format(epoch, epochs),\n",
    "                stage_progress=self.stage_progress,\n",
    "                data_getter=self.data_getter,\n",
    "                target_getter=self.target_getter,\n",
    "                get_key_metric=self.get_key_metric\n",
    "            )\n",
    "            if self.visualizer:\n",
    "                self.visualizer.update_charts(\n",
    "                    None, output_train['loss'], output_test['metric'], output_test['loss'],\n",
    "                    self.optimizer.param_groups[0]['lr'], epoch\n",
    "                )\n",
    "\n",
    "            self.metrics['epoch'].append(epoch)\n",
    "            self.metrics['train_loss'].append(output_train['loss'])\n",
    "            self.metrics['test_loss'].append(output_test['loss'])\n",
    "            self.metrics['test_metric'].append(output_test['metric'])\n",
    "\n",
    "            if self.lr_scheduler is not None:\n",
    "                if isinstance(self.lr_scheduler, ReduceLROnPlateau):\n",
    "                    self.lr_scheduler.step(output_train['loss'])\n",
    "                else:\n",
    "                    self.lr_scheduler.step()\n",
    "\n",
    "            if self.hooks[\"end_epoch\"] is not None:\n",
    "                self.hooks[\"end_epoch\"](iterator, epoch, output_train, output_test)\n",
    "\n",
    "            if (epoch + 1) % self.model_saving_frequency == 0:\n",
    "                os.makedirs(self.save_dir, exist_ok=True)\n",
    "                fname = str(epoch) + '_{:.3f}'.format(output_test['loss'])\n",
    "                torch.save(\n",
    "                    self.model.state_dict(),\n",
    "                    os.path.join(self.save_dir, self.model_name_prefix + fname) \n",
    "                )\n",
    "        return self.metrics\n",
    "\n",
    "    def fit0(self, epochs):\n",
    "        \"\"\" Fit model method.\n",
    "\n",
    "        Arguments:\n",
    "            epochs (int): number of epochs to train model.\n",
    "        \"\"\"\n",
    "        iterator = tqdm(range(epochs), dynamic_ncols=True)\n",
    "        for epoch in iterator:\n",
    "            output_train = self.hooks[\"train\"](\n",
    "                self.model,\n",
    "                self.loader_train,\n",
    "                self.loss_fn,\n",
    "                self.optimizer,\n",
    "                self.device,\n",
    "                prefix=\"[{}/{}]\".format(epoch, epochs),\n",
    "                stage_progress=self.stage_progress,\n",
    "                data_getter=self.data_getter,\n",
    "                target_getter=self.target_getter\n",
    "            )\n",
    "\n",
    "            self.metrics['epoch'].append(epoch)\n",
    "            self.metrics['train_loss'].append(output_train['loss'])\n",
    "\n",
    "            if self.lr_scheduler is not None:\n",
    "                if isinstance(self.lr_scheduler, ReduceLROnPlateau):\n",
    "                    self.lr_scheduler.step(output_train['loss'])\n",
    "                else:\n",
    "                    self.lr_scheduler.step()\n",
    "\n",
    "            if (epoch + 1) % self.model_saving_frequency == 0:\n",
    "                os.makedirs(self.save_dir, exist_ok=True)\n",
    "                fname = str(epoch)\n",
    "                torch.save(\n",
    "                    self.model.state_dict(),\n",
    "                    os.path.join(self.save_dir, self.model_name_prefix + fname) \n",
    "                )\n",
    "        return self.metrics\n",
    "\n",
    "    def register_hook(self, hook_type, hook_fn):\n",
    "        \"\"\" Register hook method.\n",
    "\n",
    "        Arguments:\n",
    "            hook_type (string): hook type.\n",
    "            hook_fn (callable): hook function.\n",
    "        \"\"\"\n",
    "        self.hooks[hook_type] = hook_fn\n",
    "\n",
    "    def _register_default_hooks(self):\n",
    "        self.register_hook(\"train\", train_hook_default)\n",
    "        self.register_hook(\"test\", test_hook_default)\n",
    "        self.register_hook(\"end_epoch\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">5. Model [5 Points]</font>\n",
    "\n",
    "Define your model in this section.\n",
    "\n",
    "**You are allowed to use any pre-trained model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Get the pretrained model:\n",
    "def pretrained_resnext50(pretrained=True, fine_tune_start=1, num_class=13):\n",
    "    resnet = models.resnext50_32x4d(pretrained=pretrained)\n",
    "    \n",
    "    if pretrained:\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    if pretrained:\n",
    "        if fine_tune_start <= 1:\n",
    "            for param in resnet.layer1.parameters():\n",
    "                param.requires_grad = True\n",
    "        if fine_tune_start <= 2:\n",
    "            for param in resnet.layer2.parameters():\n",
    "                param.requires_grad = True\n",
    "        if fine_tune_start <= 3:\n",
    "            for param in resnet.layer3.parameters():\n",
    "                param.requires_grad = True\n",
    "        if fine_tune_start <= 4:\n",
    "            for param in resnet.layer4.parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "    last_layer_in = resnet.fc.in_features\n",
    "    resnet.fc = nn.Sequential(nn.Linear(last_layer_in, 256),\n",
    "                              nn.ReLU(inplace=True),\n",
    "                              nn.Dropout(0.5),\n",
    "                              nn.Linear(256, 32),\n",
    "                              nn.ReLU(inplace=True),\n",
    "                              nn.Dropout(0.2),\n",
    "                              nn.Linear(32, num_class))\n",
    "    \n",
    "    return resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">6. Utils [5 Points]</font>\n",
    "\n",
    "Define your methods or classes which are not covered in the above sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same as those in trainer - visualizer, utils, tensorboard_visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">7. Experiment [5 Points]</font>\n",
    "\n",
    "Choose your optimizer and LR-scheduler and use the above methods and classes to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_config: configuration.SystemConfig = configuration.SystemConfig(),\n",
    "        dataset_config: configuration.DatasetConfig = configuration.DatasetConfig(),\n",
    "        dataloader_config: configuration.DataloaderConfig = configuration.DataloaderConfig(),\n",
    "        optimizer_config: configuration.OptimizerConfig = configuration.OptimizerConfig()\n",
    "    ):\n",
    "\n",
    "        # train dataloader\n",
    "        train_dataset = KenyanFood13Dataset(dataset_config.root_dir, flag=0, \n",
    "                                            split=dataset_config.split,\n",
    "                                            transform=dataset_config.train_transforms, \n",
    "                                            random_state=system_config.seed)\n",
    "        class_weight = train_dataset.get_class_weight()\n",
    "        self.loader_train = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=dataloader_config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=dataloader_config.num_workers\n",
    "        )\n",
    "        \n",
    "        # validation dataloader\n",
    "        val_dataset = KenyanFood13Dataset(dataset_config.root_dir, flag=1, \n",
    "                                          split=dataset_config.split,\n",
    "                                          transform=dataset_config.test_transforms, \n",
    "                                          random_state=system_config.seed)\n",
    "        self.loader_test = torch.utils.data.DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=dataloader_config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=dataloader_config.num_workers\n",
    "        )\n",
    "\n",
    "\n",
    "        setup_system(system_config)\n",
    "        self.model = pretrained_resnext50(pretrained=True, fine_tune_start=4)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weight))\n",
    "        self.metric_fn = AccuracyEstimator(topk=(1, ))\n",
    "        #self.optimizer = optim.SGD(\n",
    "        #    self.model.parameters(),\n",
    "        #    lr=optimizer_config.learning_rate,\n",
    "        #    weight_decay=optimizer_config.weight_decay,\n",
    "        #    momentum=optimizer_config.momentum\n",
    "        #)\n",
    "        #self.lr_scheduler = MultiStepLR(\n",
    "        #    self.optimizer, milestones=optimizer_config.lr_step_milestones, gamma=optimizer_config.lr_gamma\n",
    "        #)\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "        self.lr_scheduler = lr_scheduler.ReduceLROnPlateau(self.optimizer)\n",
    "        self.visualizer = TensorBoardVisualizer()\n",
    "\n",
    "    def run(self, trainer_config: configuration.TrainerConfig) -> dict:\n",
    "\n",
    "        device = torch.device(trainer_config.device)\n",
    "        self.model = self.model.to(device)\n",
    "        self.loss_fn = self.loss_fn.to(device)\n",
    "\n",
    "        model_trainer = Trainer(\n",
    "            model=self.model,\n",
    "            loader_train=self.loader_train,\n",
    "            loader_test=self.loader_test,\n",
    "            loss_fn=self.loss_fn,\n",
    "            metric_fn=self.metric_fn,\n",
    "            optimizer=self.optimizer,\n",
    "            lr_scheduler=self.lr_scheduler,\n",
    "            device=device,\n",
    "            data_getter=itemgetter(0),\n",
    "            target_getter=itemgetter(1),\n",
    "            stage_progress=trainer_config.progress_bar,\n",
    "            get_key_metric=itemgetter(\"top1\"),\n",
    "            visualizer=self.visualizer,\n",
    "            model_saving_frequency=trainer_config.model_saving_frequency,\n",
    "            save_dir=trainer_config.model_dir\n",
    "        )\n",
    "\n",
    "        model_trainer.register_hook(\"end_epoch\", hooks.end_epoch_hook_classification)\n",
    "        self.metrics = model_trainer.fit(trainer_config.epoch_num)\n",
    "        return self.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "class Test:\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_config: configuration.SystemConfig = configuration.SystemConfig(),\n",
    "        dataset_config: configuration.DatasetConfig = configuration.DatasetConfig(),\n",
    "        dataloader_config: configuration.DataloaderConfig = configuration.DataloaderConfig(),\n",
    "    ):\n",
    "        # test dataloader\n",
    "        test_dataset = KenyanFood13Dataset(dataset_config.root_dir, flag=2,\n",
    "                                           split=1.0,\n",
    "                                           transform=dataset_config.test_transforms, \n",
    "                                           random_state=system_config.seed)\n",
    "        self.loader_test = torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=dataloader_config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=dataloader_config.num_workers\n",
    "        )\n",
    "        self.idx_to_labels = test_dataset.idx_to_labels\n",
    "        self.model = pretrained_resnext50(pretrained=False)\n",
    "        self.model.load_state_dict(torch.load('test/model_34_0.872')) # best model saved\n",
    "        \n",
    "    def run(self, trainer_config: configuration.TrainerConfig):\n",
    "        device = torch.device(trainer_config.device)\n",
    "        \n",
    "        # set model to eval\n",
    "        self.model.eval()\n",
    "        self.model = self.model.to(device)\n",
    "        data_getter=itemgetter(0)\n",
    "        imgname_getter=itemgetter(1)\n",
    "        iterator = tqdm(self.loader_test, disable=not trainer_config.progress_bar, \n",
    "                        dynamic_ncols=True)\n",
    "        preds = []\n",
    "        imgs = []\n",
    "        for i, sample in enumerate(iterator):\n",
    "            inputs = data_getter(sample).to(device)\n",
    "            names = imgname_getter(sample)\n",
    "            with torch.no_grad():\n",
    "                predict = self.model(inputs)\n",
    "            _, predict = torch.max(predict.cpu(), dim=1)\n",
    "            #predict = [self.idx_to_labels[i] for i in predict.tolist()]\n",
    "            preds.extend(predict.tolist())\n",
    "            imgs.extend(names)\n",
    "        \n",
    "        preds = [self.idx_to_labels[i] for i in preds]\n",
    "        preds = np.c_[preds]\n",
    "        imgs = np.c_[imgs]\n",
    "        df = pd.DataFrame({'id': imgs[:,0], 'class': preds[:,0]})\n",
    "        df.to_csv('submission.csv', index=False)                \n",
    "\n",
    "def main():\n",
    "    '''Run the experiment\n",
    "    '''\n",
    "    # patch configs depending on cuda availability\n",
    "    dataloader_config, trainer_config = patch_configs(epoch_num_to_set=15)\n",
    "    dataset_config = configuration.DatasetConfig(root_dir=\".\")\n",
    "    #experiment = Experiment(dataset_config=dataset_config, dataloader_config=dataloader_config)\n",
    "    #results = experiment.run(trainer_config)\n",
    "    test = Test(dataset_config=dataset_config, dataloader_config=dataloader_config)\n",
    "    test.run(trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">8. TensorBoard Dev Scalars Log Link [5 Points]</font>\n",
    "\n",
    "Share your tensorboard scalars logs link in this section. You can also share (not mandatory) your GitHub link if you have pushed this project in GitHub. \n",
    "\n",
    "For example, [Find Project2 logs here](https://tensorboard.dev/experiment/kMJ4YU0wSNG0IkjrluQ5Dg/#scalars)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tensorboard.dev/experiment/SLygBjjvQFmUWk11b2wLPg/#scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n",
    "\n",
    "Share your Kaggle profile link here with us so that we can give points for the competition score. \n",
    "\n",
    "You should have a minimum accuracy of `75%` on the test data to get all points. If accuracy is less than `70%`, you will not get any points for the section. \n",
    "\n",
    "**You must have to submit `submission.csv` (prediction for images in `test.csv`) in `Submit Predictions` tab in Kaggle to get any evaluation in this section.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/shivsaxena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
